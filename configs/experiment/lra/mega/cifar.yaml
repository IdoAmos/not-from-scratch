# @package _global_
defaults:
  - /pipeline: cifar
  - /model: mega
  - override /scheduler: constant_warmup

scheduler:
#  num_training_steps: 250000 # 50 epochs
  num_warmup_steps: 2000 # 1 epoch

model:
  dropout: 0.
  n_layers: 8
  prenorm: true
  d_model: 160
  layer:
    _name_: mega
    d_attin: 96   # Default for LRA-Image
    d_attout: 320  # Default for LRA-Image
    d_state: 16   # Default for LRA-Image
    activation: silu
    attention_activation: laplace
    bidirectional: true
    chunk: -1
    l_max: null
    norm: batch_sync
    prenorm: True
    tie_dropout: false
    rel_pos_bias: simple
    max_positions: 4000
    ff_expand: 2  # Expansion factor for FFN
    dropout: 0.0
    drop_attin: 0.0
    drop_attout: 0.0
    drop_ffn: 0.0
    transposed: false

decoder:
  _name_: sequence_nonelinear
  mode: pool

dataset:
  grayscale: true

loader:
  batch_size: 50

optimizer:
  lr: 0.001
  weight_decay: 0.0

trainer:
  max_epochs: 200

train:
  seed: 2222
