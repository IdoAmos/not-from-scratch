# @package _global_
defaults:
  - /trainer: default
  - /loader: default
  - /dataset: etth-lm
#  - /task: regression
  - /optimizer: adamw
  - /scheduler: constant_warmup

scheduler:
  num_warmup_steps: 1000

train:
  seed: 2222
  interval: step # For cosine scheduler
  monitor: val/masked_mae
  mode: min

task:
  _name_: base
  loss: masked_mae
  metrics:
    - masked_mae
    - masked_r2
  torchmetrics: null

encoder: linear
#encoder:
#  - linear
#  - _name_: time
#    timeenc: ${dataset.timeenc}

decoder:
  _name_: sequence
  mode: last

# This dataset is super prone to overfitting so we check validation more often
trainer:
  val_check_interval: 0.2
